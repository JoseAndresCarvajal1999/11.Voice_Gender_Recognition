{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heavy-employment",
   "metadata": {},
   "source": [
    "# Voice Gender Recognition (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-chess",
   "metadata": {
    "id": "BM3L0qOLPlWb"
   },
   "source": [
    "*Import libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulation-split",
   "metadata": {
    "id": "Kqfx0amkKTRi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from modules.mlp import MultilayerPerceptron\n",
    "from modules.utils import encode_labels\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-longitude",
   "metadata": {},
   "source": [
    "*Global parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arranged-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-protein",
   "metadata": {},
   "source": [
    "*Paths*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proper-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-luther",
   "metadata": {
    "id": "bfWIeDQELcyp"
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-debut",
   "metadata": {
    "id": "moJEY1KTL50S"
   },
   "source": [
    "*Read dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rational-aside",
   "metadata": {
    "id": "INa6QwdtL4WU"
   },
   "outputs": [],
   "source": [
    "if num_features == 20:\n",
    "    dataset = pd.read_csv(data_path + 'voice.csv')\n",
    "elif num_features == 2:\n",
    "    dataset = pd.read_csv(data_path + 'embedded_voice.csv')\n",
    "else:\n",
    "    raise NotImplementedError('Unsupported scenario')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-alert",
   "metadata": {
    "id": "YKeAddFPMP8J"
   },
   "source": [
    "*Feature matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-manitoba",
   "metadata": {
    "id": "HabmREyuMRHN"
   },
   "outputs": [],
   "source": [
    "features = dataset.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-bracelet",
   "metadata": {
    "id": "-_JzhxkiMaEp"
   },
   "source": [
    "*Label vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appropriate-anaheim",
   "metadata": {
    "id": "MmlwQLokMcH8"
   },
   "outputs": [],
   "source": [
    "labels = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-uniform",
   "metadata": {
    "id": "9NotuWLXMf2e"
   },
   "source": [
    "## 2. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-superintendent",
   "metadata": {
    "id": "U5VFYdh4VYoY"
   },
   "source": [
    "*Split in train and rest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sudden-switch",
   "metadata": {
    "id": "h0yFVwPsVYod"
   },
   "outputs": [],
   "source": [
    "features_train, features_rest, labels_train, labels_rest = train_test_split(features, labels, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-familiar",
   "metadata": {
    "id": "JP0FhNN9VYog"
   },
   "source": [
    "*Split in validation and test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dated-blame",
   "metadata": {
    "id": "cx2SeNdOVYoh"
   },
   "outputs": [],
   "source": [
    "features_val, features_test, labels_val, labels_test = train_test_split(features, labels, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-growth",
   "metadata": {
    "id": "uauASaLAVYoi"
   },
   "source": [
    "*Scale features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "senior-paintball",
   "metadata": {
    "id": "-B2sSbf9VYok"
   },
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "features_train = sc.fit_transform(features_train)\n",
    "features_val = sc.transform(features_val)\n",
    "features_test = sc.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-plumbing",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-cemetery",
   "metadata": {},
   "source": [
    "*Build architectures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "closed-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = []\n",
    "\n",
    "def build_architectures(architecture):\n",
    "    if len(architecture) >= 1:\n",
    "        architecture = architecture\n",
    "        architectures.append(architecture)\n",
    "    if len(architecture) < 3:\n",
    "        for i in range(5, 10 + 1):\n",
    "            build_architectures(architecture + [i])\n",
    "\n",
    "build_architectures([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-contributor",
   "metadata": {},
   "source": [
    "*Compute performance metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minor-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels_eval, labels_pred, labels_scores):\n",
    "    metrics = {}\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_eval, labels_pred).ravel()\n",
    "    metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "    metrics['sensitivity'] = tp / (tp + fn)\n",
    "    metrics['specificity'] = tn / (tn + fp)\n",
    "    metrics['fpr'], metrics['tpr'], _ = roc_curve(labels_eval, labels_scores[:, 1])\n",
    "    metrics['auc'] = auc(metrics['fpr'], metrics['tpr'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-gathering",
   "metadata": {},
   "source": [
    "*Assess classifier's performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "protecting-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_performance(classifier, features_eval, labels_eval):\n",
    "    labels_scores = classifier.predict(features_eval)\n",
    "    labels_pred = np.argmax(labels_scores, axis=1)\n",
    "    return compute_metrics(labels_eval, labels_pred, labels_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-manner",
   "metadata": {},
   "source": [
    "*Build scenario id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operating-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scenario_id(learning_rate, architecture):\n",
    "    scheme = '-'.join(str(layer) for layer in architecture)\n",
    "    return '{:.2f}-{}'.format(learning_rate, scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-table",
   "metadata": {},
   "source": [
    "*Arrange gradients' mean norms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polar-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_gradients_mean_norms(gradients_mean_norms, architecture):\n",
    "    return {layer + 1 : gradients_mean_norms[:, layer] for layer in range(len(architecture) - 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-species",
   "metadata": {},
   "source": [
    "*Encode training labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "genuine-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_train = encode_labels(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-violation",
   "metadata": {},
   "source": [
    "*Train models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complete-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 258/258 [12:32:35<00:00, 175.02s/it]\n"
     ]
    }
   ],
   "source": [
    "brains_val = {}\n",
    "brains_test = {}\n",
    "brains_errors = {}\n",
    "brains_gradients = {}\n",
    "\n",
    "# Sweep over architectures\n",
    "for architecture in tqdm(architectures):\n",
    "    # Select architecture\n",
    "    architecture = [features.shape[1]] + architecture + [encoded_labels_train.shape[1]]\n",
    "    # Sweep over learning rates\n",
    "    for eta in [0.2, 0.5, 0.9]:\n",
    "        # Scenario id\n",
    "        scenario_id = build_scenario_id(eta, architecture)\n",
    "        # Build brain\n",
    "        brain = MultilayerPerceptron(architecture, learning_rate=eta)\n",
    "        # Train brain\n",
    "        network_avg_errors, gradients_mean_norms = brain.fit(features_train, encoded_labels_train)\n",
    "        # Save training outcomes\n",
    "        brains_errors[scenario_id] = network_avg_errors\n",
    "        brains_gradients[scenario_id] = arrange_gradients_mean_norms(gradients_mean_norms, architecture)\n",
    "        # Evaluate classifier's performance in the validation set\n",
    "        metrics_val = assess_performance(brain, features_val, labels_val)\n",
    "        brains_val[scenario_id] = metrics_val\n",
    "        # Evaluate classifier's performance in the test set\n",
    "        metrics_test = assess_performance(brain, features_test, labels_test)\n",
    "        brains_test[scenario_id] = metrics_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-harvey",
   "metadata": {},
   "source": [
    "*Encase results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lucky-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "brains_val = pd.DataFrame(brains_val).T\n",
    "brains_test = pd.DataFrame(brains_test).T\n",
    "brains_errors = pd.DataFrame(brains_errors).T\n",
    "brains_gradients = pd.DataFrame(brains_gradients).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-somewhere",
   "metadata": {},
   "source": [
    "*Persist results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "established-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "brains_val.to_csv(data_path + 'brains_val_{}d.csv'.format(num_features))\n",
    "brains_test.to_csv(data_path + 'brains_test_{}d.csv'.format(num_features))\n",
    "brains_errors.to_csv(data_path + 'brains_errors_{}d.csv'.format(num_features))\n",
    "brains_gradients.to_csv(data_path + 'brains_gradients_{}d.csv'.format(num_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('ml': conda)",
   "language": "python",
   "name": "python38364bitmlconda1a1aa23d0e0b4a48be71e958a1e0752d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
